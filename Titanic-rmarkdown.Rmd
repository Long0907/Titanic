---
title: "Titanic"
author: "Long Zheng"
date: "12/4/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

## __Use machine learning to create a model that predicts which passengers survived the Titanic shipwreck__


#required packages
```{r, include=FALSE}
library(tidyverse)
library(ROCit)
source("plot_resid_lev_logistic.R")
```

#load the trainning dataset and explore.
```{r}
train<- read_csv("dataset/train.csv")
```


```{r EDA}
#drop cabin column, because of too many missing data 687/891. 
#train <- train[!colnames(train)=='Cabin']
# make sex and embarked as factor variables with levels
train<-
  train %>% 
  mutate(Sex.f = as.factor(Sex),
         Embarked.f = factor(Embarked),
         Pclass.f = factor(Pclass),
         Survived = factor(Survived))
#get an overview of the data
t1<-
train %>% 
  select(Pclass,Age,SibSp,Parch,Fare,Sex.f,Embarked.f) %>% 
  GGally::ggpairs()
#too much missing data in cabin (687/891). 
t1 <- data.frame(t1)

# survival distribution  
hist(train$Survived)
# age
hist(train$Age,xlab = "Age",ylab = "count", main = "")
# sex
hist(train$Sex2,xlab = "Gender(male =1)",ylab = "count", main = "")
# fare
hist(train$Fare)
```

```{r}
train %>% 
  group_by(Embarked) %>% 
  summarise(age = mean(Age,na.rm = T),
            fare = mean(Fare,na.rm = T))

```

```{r}
# Access linearity
library(mfp)
mfp(Survived~fp(Age),family=binomial,data = train)
mfp(Survived~fp(Fare),family=binomial,data = train)
# cut fare variable 
train<-
  train %>% 
  mutate(fare_cut = quantile(Fare,))

# univariable analysis
univ_Pclass<-glm(Survived~Pclass, family = binomial, data = train)
summary(univ_Pclass)
univ_Age<-glm(Survived~Age, family = binomial, data = train)
summary(univ_Age)
univ_Sex<-glm(Survived~Sex, family = binomial, data = train)
summary(univ_Sex)
univ_SibSp<-glm(Survived~SibSp, family = binomial, data = train)
summary(univ_SibSp) # not statistic significant
univ_Parch<-glm(Survived~Parch, family = binomial, data = train)
summary(univ_Parch)
univ_Embarked.f<-glm(Survived~Embarked.f, family = binomial, data = train)
summary(univ_Embarked.f)
univ_Fare<-glm(Survived~Fare, family = binomial, data = train)
summary(univ_Fare)

#use glmulti to obtain a best model.
best_subset_att <-
  glmulti::glmulti(Survived~Age+factor(Pclass)+Sex.f+Fare+Parch+Embarked.f, data = train,level=1, family = binomial, crit="aicc", confsetsize=128)
```

#use glmulti to obtain a best model.
```{r}
best_subset_att <-
  glmulti::glmulti(Survived ~ I(Age^(-1))+factor(Pclass)+Sex.f+Fare+SibSp+Parch+Embarked.f, 
          data = train,
          level=1, family = binomial, crit="aicc", confsetsize=128)

best_model <- 
  summary(best_subset_att)$bestmodel %>% glm(., data = train, family = binomial)

glm(Survived~+factor(Pclass)+Sex.f+Age+SibSp,family = binomial,data = train) %>% summary()
# add interaction 
glm(Survived~1+factor(Pclass)+Sex.f+Age+SibSp,family = binomial,data = train) %>% summary()
```



```{r}
#Assess godness of fit.
ResourceSelection::hoslem.test(best_model$y, fitted(best_model), g=20)

#Assess residual and leverage
plot_resid_lev_logistic(best_model)

# Classification
DescTools::Conf(best_model, pos = 1)

best_model.p <-
  tibble(
    pred_p = best_model$fitted.values,
    y = best_model$y
  )

best_model.p %>%
  ggplot(aes(x = pred_p)) + 
  facet_wrap(~y) +
  geom_histogram() +
  geom_vline(xintercept = .5, color = "red")

# Accuracy
best_model.roc <- 
  ROCit::measureit(score = best_model$fitted.values, 
                   class = best_model$y,
                   measure = c("ACC", "SENS", "SPEC"))

tibble(
  Cutoff = best_model.roc$Cutoff,
  ACC = best_model.roc$ACC
) %>%
ggplot(aes(x = Cutoff, y = ACC)) +
  geom_point() +
  geom_line()

# ROC Curve
tibble(
  Cutoff = best_model.roc$Cutoff,
  SENS = best_model.roc$SENS,
  SPEC = best_model.roc$SPEC
) %>%
  pivot_longer(., cols = c("SENS", "SPEC"), values_to = "value", names_to = "metric") %>%
  ggplot(aes(x = Cutoff, y = value, color = metric)) +
  geom_point() + 
  geom_line()

tibble(
  Cutoff = best_model.roc$Cutoff,
  SENS = best_model.roc$SENS,
  SPEC = best_model.roc$SPEC,
  SUM = SENS + SPEC
) %>%
  arrange(-SUM, -SENS, -SPEC)
  
roc_empirical <- 
  rocit(score = best_model$fitted.values, class = best_model$y)
plot(roc_empirical, YIndex = F)
roc_empirical
summary(roc_empirical)
ciAUC(roc_empirical)

OptimalCutpoints::optimal.cutpoints(X = "pred_p", status = "y", 
                  data = data.frame(best_model.p), 
                  methods = c("Youden", "MaxSpSe", "MaxProdSpSe"), tag.healthy = 0)


library(plotROC)
best_model.p %>%
  ggplot(aes(m = pred_p, d = y)) + 
  geom_roc(n.cuts=0,labels=FALSE) + 
  style_roc(theme = theme_grey, xlab = "1 - Specificity", ylab = "Sensitivity") +
  geom_abline(slope = 1, intercept = 0)


```

#test prediction based on the selected logistic model
```{r}
#######Model prediction
test <- read_csv("dataset/test.csv")

# make sex and embarked as factor variables with levels
test<-
  test %>% 
  mutate(Sex.f = as.factor(Sex),
         Embarked.f = factor(Embarked),
         Pclass.f = factor(Pclass),
         Survived = factor(Survived)
         )
#test prediction.
test_prediction <- tibble(pred.test = predict(best_model, test, type = "response"))

#cut point 0.5.
test_prediction <-test_prediction %>% 
  mutate(Survived = if_else(pred.test>0.5, 1, 0),
         PassengerId = test$PassengerId)

submission <- 
  test_prediction %>% 
  select(PassengerId, Survived)

write.csv(submission, "Titanic-submission.csv", row.names = FALSE)
```


```{r univariable analysis}
univ_embk<-glm(Survived~Embarked2, family = binomial, data = train)
summary(univ_embk)
univ_Pclass<-glm(Survived~Pclass, family = binomial, data = train)
summary(univ_Pclass)
```


#####try use other machine learning methods. randomForest classification.
```{r}
train_random <- train %>% 
  select(Survived, Age,Pclass.f, Sex.f,Fare,SibSp,Parch,Embarked.f)

#add one column, if Age == NA, then 1. else 0. 
age_na <- is.na(train_random$Age)

train_random[!age_na,] %>% 
  psych::describe()
age_model <- lm(Age~Pclass.f+I(log(Fare+0.01))+SibSp, data=train_random[!age_na,])
summary(age_model)


pred_age <- predict(age_model, train_random)

  
impute_data<-train_random %>% 
  mutate(Age = if_else(is.na(Age), pred_age, Age))
  
impute_data<- na.omit(impute_data)

rf_model1 <- randomForest(Survived~.,
                   data = impute_data, importance=TRUE, mtry=3)
rf_model1


predict(rf_model1, test_set, type = "class")


```

#test prediction based on the selected randomForest model
```{r}
#######Model prediction
test <- read_csv("dataset/test.csv")

# make sex and embarked as factor variables with levels
test<-
  test %>% 
  mutate(Sex.f = as.factor(Sex),
         Embarked.f = factor(Embarked),
         Pclass.f = factor(Pclass))
#impute NAs with predicted Age
pred_age_test <- predict(age_model, test)

impute_data_test<-test %>% 
  mutate(Age = if_else(is.na(Age), pred_age_test, Age))


```{r univariable analysis}
univ_embk<-glm(Survived~Embarked2, family = binomial, data = train)
summary(univ_embk)
univ_Pclass<-glm(Survived~Pclass, family = binomial, data = train)
summary(univ_Pclass)
#test prediction.
test_prediction <- tibble(pred.test = predict(rf_model1, impute_data_test, type = "class"))

#cut point 0.5.
test_prediction <-test_prediction %>% 
  mutate(PassengerId = test$PassengerId,
         Survived = pred.test)

submission <- 
  test_prediction %>% 
  select(PassengerId, Survived)

write.csv(submission, "Titanic-submission.csv", row.names = FALSE)
```








